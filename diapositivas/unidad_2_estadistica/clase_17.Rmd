---
title: "Regresión lineal aplicada"
subtitle: "Unidad 2: Estadística Básica y Aplicada"
author: "Nicolás Sidicaro"
date: "Octubre 2025"
institution: "FCE-UBA"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  fig.retina = 3,
  out.width = "100%",
  cache = FALSE
)

# Cargar paquetes
library(wooldridge)
library(tidyverse)
library(car)
library(lmtest)
library(sandwich)
library(stargazer)
library(knitr)
library(kableExtra)
```

class: inverse, center, middle

# Bloque 1
## Fundamentos de Regresión Lineal

---

# ¿Qué estamos haciendo con regresión?

.pull-left[
**Objetivo principal:**
- Modelar relaciones entre variables
- Describir asociaciones
- Hacer predicciones

**No necesariamente:**
- Establecer causalidad
- Demostrar que X "causa" Y
]

.pull-right[
```{r echo=FALSE, fig.height=5}
set.seed(123)
x <- rnorm(100)
y <- 2 + 3*x + rnorm(100)
plot(x, y, pch=19, col="steelblue", 
     main="Relación entre X e Y",
     xlab="Variable X", ylab="Variable Y")
abline(lm(y~x), col="red", lwd=2)
```
]

---

# ⚠️ Correlación ≠ Causalidad

.pull-left[
**Correlaciones espurias:**

Ejemplos clásicos:
- Consumo de helado y ahogamientos
- Número de películas de Nicolas Cage y ahogamientos en piscinas
- Divorcio en Maine y consumo de margarina

**Mensaje clave:** Una relación estadística fuerte NO implica que una variable cause la otra
]

.pull-right[
```{r echo=FALSE, fig.height=5}
años <- 2000:2009
helado <- c(2.5, 3.2, 3.8, 4.1, 3.9, 4.5, 4.8, 4.3, 4.6, 5.1)
ahogamientos <- c(120, 145, 165, 175, 168, 185, 195, 180, 188, 205)
plot(helado, ahogamientos, pch=19, col="darkred", cex=1.5,
     main="Ejemplo: Helado vs Ahogamientos",
     xlab="Ventas de helado", ylab="Ahogamientos")
abline(lm(ahogamientos~helado), col="blue", lwd=2)
```
]

---

# El Modelo de Regresión Lineal Simple

$$Y_i = \beta_0 + \beta_1 X_i + u_i$$

**Componentes:**
- $Y_i$: Variable dependiente (lo que queremos explicar)
- $X_i$: Variable independiente (explicativa)
- $\beta_0$: Intercepto (valor de Y cuando X=0)
- $\beta_1$: Pendiente (efecto marginal de X sobre Y)
- $u_i$: Error o residuo (lo que no podemos explicar)

--

**MCO (Mínimos Cuadrados Ordinarios):**
- Minimiza la suma de errores al cuadrado: $\min \sum u_i^2$
- Encuentra la "mejor" línea que pasa por los datos

---

# Primer Ejemplo en R: Salarios
```{r}
# Cargar datos de salarios
data(wage1)
head(wage1[, c("wage", "educ", "exper", "tenure")], 8)
```

---

# Modelo Simple: Salario ~ Educación
```{r}
# Estimar modelo
modelo1 <- lm(wage ~ educ, data = wage1)
summary(modelo1)
```

---

# Interpretación del Output
```{r echo=FALSE}
summary(modelo1)$coefficients
```

**Interpretación:**

- **Intercepto (β₀):** -0.90 → Una persona con 0 años de educación ganaría -$0.90/hora (no tiene sentido real, es extrapolación)

- **Educación (β₁):** 0.54 → Por cada año adicional de educación, el salario aumenta en $0.54/hora **en promedio**

- **P-valores < 0.001:** Ambos coeficientes son altamente significativos (***) 

- **R² = 0.165:** La educación explica solo el 16.5% de la variación en salarios

---

# Visualización del Modelo
```{r echo=FALSE, fig.height=6}
ggplot(wage1, aes(x = educ, y = wage)) +
  geom_point(alpha = 0.5, color = "steelblue") +
  geom_smooth(method = "lm", color = "red", se = TRUE) +
  labs(title = "Salario vs Educación",
       x = "Años de Educación",
       y = "Salario por hora ($)") +
  theme_minimal(base_size = 14)
```

---

# Modelos Logarítmicos: ¿Por qué?

**Problema:** Las relaciones económicas rara vez son lineales

**Soluciones:** Transformaciones logarítmicas

| Modelo | Ecuación | Interpretación de β₁ |
|--------|----------|---------------------|
| **Lin-Lin** | $Y = \beta_0 + \beta_1 X$ | Cambio en unidades |
| **Log-Lin** | $\log(Y) = \beta_0 + \beta_1 X$ | $\beta_1 \times 100$% cambio en Y por unidad de X |
| **Lin-Log** | $Y = \beta_0 + \beta_1 \log(X)$ | $\beta_1/100$ unidades de Y por 1% cambio en X |
| **Log-Log** | $\log(Y) = \beta_0 + \beta_1 \log(X)$ | Elasticidad: β₁% cambio en Y por 1% cambio en X |

---

# Ejemplo: Modelo Log-Lin
```{r}
# Modelo con logaritmo del salario
modelo_log <- lm(log(wage) ~ educ, data = wage1)
summary(modelo_log)$coefficients
```

--

**Interpretación:**

- **β₁ = 0.083** → Por cada año adicional de educación, el salario aumenta aproximadamente **8.3%** 

- Cálculo: $e^{0.083} - 1 = 0.0865$ ≈ 8.65%

- Este modelo suele ajustar mejor para salarios (R² = 0.186 vs 0.165)

---

# Comparación Visual: Lineal vs Log

.pull-left[
```{r echo=FALSE, fig.height=5}
ggplot(wage1, aes(x = educ, y = wage)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "Modelo Lineal", x = "Educación", y = "Salario") +
  theme_minimal()
```
]

.pull-right[
```{r echo=FALSE, fig.height=5}
ggplot(wage1, aes(x = educ, y = log(wage))) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Modelo Log-Lineal", x = "Educación", y = "Log(Salario)") +
  theme_minimal()
```
]

---

# Modelo Log-Log: Elasticidades
```{r}
# Precios de casas y tamaño
data(hprice1)

# Modelo log-log
modelo_elasticidad <- lm(log(price) ~ log(sqrft), data = hprice1)
summary(modelo_elasticidad)$coefficients
```

--

**Interpretación:**

- **β₁ = 0.70** → Si el tamaño de la casa aumenta 1%, el precio aumenta 0.70% (elasticidad)

- Útil para comparar variables en diferentes escalas

---

class: inverse, center, middle

# Bloque 2
## Regresión Múltiple y Variables Categóricas

---

# Regresión Múltiple

$$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + ... + \beta_k X_{ki} + u_i$$

**Ventajas:**
- Controlar por múltiples factores simultáneamente
- Reducir el sesgo por variables omitidas
- Interpretación **ceteris paribus** (manteniendo todo lo demás constante)

--

**Interpretación de βⱼ:**
- El cambio en Y cuando Xⱼ aumenta en una unidad, **manteniendo las demás variables constantes**

---

# Ejemplo: Salario con Múltiples Variables
```{r}
# Modelo múltiple
modelo_multiple <- lm(wage ~ educ + exper + tenure, data = wage1)
summary(modelo_multiple)
```

---

# Interpretación del Modelo Múltiple
```{r echo=FALSE}
summary(modelo_multiple)$coefficients
```

**Interpretaciones (ceteris paribus):**

- **Educación:** Por cada año adicional de educación, el salario aumenta $0.60/hora, manteniendo experiencia y antigüedad constantes

- **Experiencia:** Por cada año de experiencia, el salario aumenta $0.014/hora

- **Antigüedad (tenure):** Por cada año en el trabajo actual, el salario aumenta $0.17/hora

--

**Nota:** Los coeficientes cambiaron respecto al modelo simple (educación era 0.54, ahora es 0.60)

---

# Bondad de Ajuste: R² y R² Ajustado
```{r echo=FALSE}
cat("R² =", round(summary(modelo_multiple)$r.squared, 4), "\n")
cat("R² ajustado =", round(summary(modelo_multiple)$adj.r.squared, 4))
```

**R² (coeficiente de determinación):**
- Proporción de la varianza de Y explicada por el modelo
- Rango: 0 a 1
- **Problema:** Siempre aumenta al agregar variables (aunque no sean relevantes)

--

**R² ajustado:**
- Penaliza por agregar variables
- Puede disminuir si agregamos variables irrelevantes
- **Mejor para comparar modelos con distinto número de variables**

---

# Tests de Significancia

**Test t individual:** ¿Es βⱼ significativamente distinto de cero?
- $H_0$: $\beta_j$ = 0 (la variable no tiene efecto)
- $H_i$: $\beta_j$ != 0 (la variable sí tiene efecto)
- Miramos el p-valor: 
  - p < 0.01 → $***$ (muy significativo)
  - p < 0.05 → $**$ (significativo)
  - p < 0.10 → $*$ (marginalmente significativo)

--

**Test F global:** ¿El modelo completo es significativo?
- $H_0$ : $\beta_1 = \beta_2 = ... = \beta_k = 0$ (ninguna variable importa)
- $H_1$ : Al menos un $\beta_k$ !=  0
```{r echo=FALSE}
f_stat <- summary(modelo_multiple)$fstatistic
p_value <- pf(f_stat[1], f_stat[2], f_stat[3], lower.tail = FALSE)
cat("F-statistic:", round(f_stat[1], 2), "con p-valor < 0.001")
```

---

# Variables Dicotómicas (Dummies)

**¿Qué son?**
- Variables que toman valores 0 o 1
- Representan categorías: género, región, tratamiento, etc.

**Codificación:**
```{r}
wage1 <- wage1 %>%
  mutate(mujer = ifelse(female == 1, 1, 0))

# Ver distribución
table(wage1$mujer)
```

---

# Ejemplo: Brecha de Género Salarial
```{r}
# Modelo con dummy de género
modelo_genero <- lm(wage ~ educ + exper + mujer, data = wage1)
summary(modelo_genero)$coefficients
```

--

**Interpretación:**

- **mujer = -1.81** → Las mujeres ganan $1.81/hora **menos** que los hombres, manteniendo educación y experiencia constantes

- Altamente significativo (p < 0.001)

---

# Brecha de Género en Modelo Log-Lineal
```{r}
# Modelo log para interpretar en porcentajes
modelo_genero_log <- lm(log(wage) ~ educ + exper + mujer, data = wage1)
summary(modelo_genero_log)$coefficients
```

--

**Interpretación:**

- **mujer = -0.297** → Las mujeres ganan aproximadamente **29.7% menos**, manteniendo educación y experiencia constantes

- Más fácil de comunicar que diferencias absolutas

---

# Variables Categóricas con Múltiples Categorías

**Regla m-1:** Si una variable tiene m categorías, incluimos m-1 dummies

**Ejemplo:** Ocupación (8 categorías en wage1)
```{r}
# Ver ocupaciones
table(wage1$occupation)
```

--

**¿Por qué m-1?**
- Evitar multicolinealidad perfecta
- La categoría omitida es la **categoría base** (referencia)
- Las dummies se interpretan respecto a la base

---

# Ejemplo: Ocupación y Salarios
```{r}
wage1 <- wage1 %>% 
  mutate(ocupacion = case_when(construc == 1 ~ 'Construccion',
                               ndurman == 1 ~ 'Industria',
                               trcommpu == 1 ~ 'Transporte y comunicaciones',
                               trade == 1 ~ 'Comercio',
                               services == 1 ~ 'Servicios',
                               TRUE ~ 'Otros servicios'
                               ),
         ocupacion_f = factor (ocupacion))
modelo_ocupacion <- lm(log(wage) ~ educ + exper + ocupacion_f, data = wage1)
```
```{r echo=FALSE}
summary(modelo_ocupacion)
```

**Interpretación:** Cada coeficiente compara esa ocupación con la categoría base ("Comercio")

---

class: inverse, center, middle

# Bloque 3
## Supuestos y Diagnósticos

---

# Supuestos de Gauss-Markov

Para que MCO sea el **Mejor Estimador Lineal Insesgado (MELI/BLUE):**

1. **Linealidad en parámetros:** El modelo es lineal en los β

2. **Exogeneidad:** $E[u|X] = 0$ (el error no está correlacionado con X)

3. **No multicolinealidad perfecta:** Las X no están perfectamente correlacionadas

4. **Homocedasticidad:** $Var(u|X) = \sigma^2$ (varianza constante del error)

5. **No autocorrelación:** $Cov(u_i, u_j) = 0$ para i ≠ j

--

**¿Qué pasa si se violan?**
- Estimadores dejan de ser óptimos
- Pueden ser sesgados o ineficientes
- Necesitamos diagnósticos y correcciones

---

# PROBLEMA 1: Multicolinealidad

**¿Qué es?**
- Alta correlación entre variables independientes
- Las X están "diciendo lo mismo"

**Síntomas:**
- R² alto pero pocos coeficientes significativos
- Coeficientes con signos inesperados
- Errores estándar muy grandes
- Coeficientes muy sensibles a pequeños cambios en datos

--

**Consecuencia:**
- Estimadores siguen siendo insesgados
- Pero tienen varianzas infladas → dificulta detectar efectos reales

---

# Detección: Matriz de Correlación
```{r fig.height=5}
# Seleccionar variables numéricas
vars_numericas <- wage1 %>% select(wage, educ, exper, tenure)

# Matriz de correlación
cor(vars_numericas) %>% round(3)
```

**Interpretación:**
- Experiencia y antigüedad tienen correlación moderada (0.459)
- Educación tiene baja correlación con las demás (bueno)

---

# Detección: Factor de Inflación de Varianza (VIF)
```{r}
# Calcular VIF
vif(modelo_multiple)
```

--

**Regla general:**
- VIF = 1: No correlación
- VIF entre 1-5: Correlación moderada (aceptable)
- VIF entre 5-10: Correlación alta (preocupante)
- VIF > 10: Multicolinealidad severa

**En nuestro caso:** No hay problema serio de multicolinealidad

---

# Ejemplo con Multicolinealidad Alta
```{r}
# Crear variable altamente correlacionada
wage1$exper_mas_tenure <- wage1$exper + wage1$tenure + rnorm(nrow(wage1), 0, 0.5)

modelo_multicol <- lm(wage ~ educ + exper + tenure + exper_mas_tenure, data = wage1)
```
```{r echo=FALSE}
summary(modelo_multicol)$coefficients
```
```{r}
vif(modelo_multicol)
```

**¡VIF altísimos!** exper_mas_tenure está causando multicolinealidad

---

# Soluciones a Multicolinealidad

**1. Eliminar una de las variables correlacionadas**
```{r}
# Quitar la variable problemática
modelo_sin_multicol <- lm(wage ~ educ + exper + tenure, data = wage1)
```

**2. Transformar variables**
- Crear ratios (ej: PBI per cápita en vez de PBI y Población)
- Usar diferencias

**3. Aceptarlo (si el objetivo es predicción)**
- Si no nos interesa interpretar coeficientes individuales
- Si queremos solo predecir Y

**4. Obtener más datos**
- A veces el problema es muestra pequeña

---

# PROBLEMA 2: Heteroscedasticidad

**¿Qué es?**
- La varianza del error NO es constante
- $Var(u|X) = \sigma^2_i$ (varía con i)

**¿Por qué importa?**
- Estimadores de MCO siguen siendo **insesgados**
- Pero los **errores estándar están mal calculados**
- Consecuencia: inferencia incorrecta (p-valores erróneos)

--

**Causas comunes:**
- Variables con escalas muy diferentes
- Relaciones que se amplifican (ej: ingreso alto → mayor variación en consumo)
- Variables omitidas

---

# Detección Visual: Gráfico de Residuos
```{r fig.height=5.5}
# Residuos vs valores ajustados
plot(modelo_multiple, which = 1, pch = 19)
```

**¿Qué buscamos?** Patrones en la dispersión (forma de embudo, curvas, etc.)

---

# Interpretación de Gráficos de Residuos
```{r echo=FALSE, fig.height=6}
par(mfrow = c(1, 2))
# Homocedasticidad
set.seed(123)
x_homo <- rnorm(100)
y_homo <- 2 + 3*x_homo + rnorm(100)
plot(fitted(lm(y_homo~x_homo)), residuals(lm(y_homo~x_homo)), 
     pch=19, col="steelblue", alpha=0.5,
     main="✓ Homocedasticidad", xlab="Fitted", ylab="Residuals")
abline(h=0, col="red", lwd=2)

# Heteroscedasticidad
x_hetero <- rnorm(100)
y_hetero <- 2 + 3*x_hetero + rnorm(100, sd=abs(x_hetero))
plot(fitted(lm(y_hetero~x_hetero)), residuals(lm(y_hetero~x_hetero)), 
     pch=19, col="darkred",alpha= 0.5,
     main="✗ Heteroscedasticidad", xlab="Fitted", ylab="Residuals")
abline(h=0, col="red", lwd=2)
```

---

# Tests Formales de Heteroscedasticidad

**Test de Breusch-Pagan:**
```{r}
# Test de Breusch-Pagan
bptest(modelo_multiple)
```

--

**Interpretación:**
- H₀: Homocedasticidad (varianza constante)
- H₁: Heteroscedasticidad
- p-valor = 0.014 < 0.05 → **Rechazamos H₀**
- **Hay evidencia de heteroscedasticidad**

---

# Test de White (más general)
```{r}
# Test de White (incluye términos cuadráticos)
bptest(modelo_multiple, ~ educ + exper + tenure + 
         I(educ^2) + I(exper^2) + I(tenure^2), data = wage1)
```

--

**Interpretación:**
- Test más general que Breusch-Pagan
- También detecta formas no lineales de heteroscedasticidad
- p-valor < 0.05 → Confirmamos heteroscedasticidad

---

# SOLUCIÓN: Errores Robustos (Errores de White)

**Idea:** Corregir los errores estándar sin cambiar los coeficientes
```{r}
# Modelo original (errores normales)
summary(modelo_multiple)$coefficients
```

---

# Comparación: Errores Normales vs Robustos
```{r}
# Errores robustos usando sandwich
coeftest(modelo_multiple, vcov = vcovHC(modelo_multiple, type = "HC1"))
```

**¿Qué cambió?**
- Los coeficientes son idénticos
- Los errores estándar son diferentes
- Los p-valores cambian ligeramente
- La significancia puede cambiar en casos borderline

---

# Visualización de la Diferencia
```{r echo=FALSE}
# Crear tabla comparativa
normal_se <- sqrt(diag(vcov(modelo_multiple)))
robust_se <- sqrt(diag(vcovHC(modelo_multiple, type = "HC1")))

comparacion <- data.frame(
  Variable = names(coef(modelo_multiple)),
  Coeficiente = coef(modelo_multiple),
  SE_Normal = normal_se,
  SE_Robusto = robust_se,
  Diferencia = ((robust_se - normal_se)/normal_se * 100)
)

kable(comparacion, digits = 3, 
      caption = "Comparación de Errores Estándar") %>%
  kable_styling(font_size = 14)
```

**Los errores robustos son generalmente mayores** (más conservadores)

---

# ¿Cuándo Usar Errores Robustos?

**Recomendación práctica:**

✅ **SIEMPRE úsalos en datos de corte transversal**
- Es la práctica estándar en economía
- No se pierde nada si hay homocedasticidad
- Proteges tu inferencia si hay heteroscedasticidad
```{r eval=FALSE}
# Tu workflow estándar debería ser:
modelo <- lm(Y ~ X1 + X2, data = datos)
coeftest(modelo, vcov = vcovHC(modelo, type = "HC1"))
```

--

⚠️ **No son necesarios para:**
- Series de tiempo (hay otras correcciones)
- Datos de panel (efectos fijos/aleatorios)

---

# PROBLEMA 3: Endogeneidad (Mención Breve)

**¿Qué es?**
- Violación del supuesto de exogeneidad: $E[u|X] \neq 0$
- Las X están correlacionadas con el error

**Causas principales:**

1. **Variables omitidas:** Olvidamos incluir Z relevante que se correlaciona con X
2. **Error de medición:** X está medido con error
3. **Simultaneidad:** Y también afecta a X (causalidad reversa)

--

**Consecuencia:** Estimadores son **SESGADOS** (no solo ineficientes)

**En esta clase:** Solo lo reconocemos, no lo resolvemos
- Solución requiere Variables Instrumentales (otra clase)
- Nuestro β describe asociación, no causalidad

---

# Ejemplo Conceptual: Variable Omitida

**Modelo verdadero:**
$$Salario = \beta_0 + \beta_1 Educacion + \beta_2 Habilidad + u$$

**Modelo estimado (habilidad no observable):**
$$Salario = \alpha_0 + \alpha_1 Educacion + e$$

--

**Problema:**
- Habilidad está en el error (e)
- Habilidad se correlaciona con Educación (personas más hábiles estudian más)
- $Cov(Educacion, e) \neq 0$ → ¡Endogeneidad!
- $\alpha_1$ está **sesgado** (probablemente sobreestima el efecto de educación)

--

**Mensaje clave:** Nuestras regresiones describen asociaciones. La causalidad requiere más supuestos o diseño experimental.

---

class: inverse, center, middle

# Bloque 4
## Workflow y Buenas Prácticas

---

# Flujo de Trabajo Recomendado

**1. EXPLORACIÓN**
```{r eval=FALSE}
# Estadísticas descriptivas
summary(datos)
# Correlaciones
cor(datos)
# Gráficos exploratorios
plot(datos)
```

**2. ESPECIFICACIÓN**
- Elegir variables relevantes (teoría económica)
- Decidir forma funcional (lineal, log, cuadrática)

**3. ESTIMACIÓN**
```{r eval=FALSE}
modelo <- lm(Y ~ X1 + X2, data = datos)
```

---

# Flujo de Trabajo (continuación)

**4. DIAGNÓSTICO**
```{r eval=FALSE}
# Gráficos de residuos
plot(modelo)
# Multicolinealidad
vif(modelo)
# Heteroscedasticidad
bptest(modelo)
```

**5. CORRECCIÓN** (si es necesario)
```{r eval=FALSE}
# Errores robustos
coeftest(modelo, vcov = vcovHC(modelo, type = "HC1"))
```

**6. INTERPRETACIÓN Y REPORTE**
- Presentar resultados de forma clara
- Ser honesto sobre limitaciones

---

# Selección Entre Modelos
```{r}
# Tres modelos candidatos
m1 <- lm(log(wage) ~ educ, data = wage1)
m2 <- lm(log(wage) ~ educ + exper, data = wage1)
m3 <- lm(log(wage) ~ educ + exper + tenure, data = wage1)

# Comparar con criterios de información
data.frame(
  Modelo = c("M1: educ", "M2: educ + exper", "M3: educ + exper + tenure"),
  R2_adj = c(summary(m1)$adj.r.squared, 
             summary(m2)$adj.r.squared, 
             summary(m3)$adj.r.squared),
  AIC = c(AIC(m1), AIC(m2), AIC(m3)),
  BIC = c(BIC(m1), BIC(m2), BIC(m3))
) %>% 
  kable(digits = 3) %>%
  kable_styling(font_size = 14)
```

**Menor AIC/BIC es mejor** → M3 es el mejor modelo

---

# Test de Ramsey (RESET)

**¿Está bien especificado el modelo?**
```{r}
# Test RESET
resettest(modelo_multiple, power = 2:3)
```

--

**Interpretación:**
- H₀: El modelo está correctamente especificado
- H₁: Hay problemas de especificación (forma funcional incorrecta, **variables omitidas -depende el manual-**)
- p-valor < 0.05 → Posible problema de especificación
- **Acción:** Considerar transformaciones o variables adicionales

---

# Reporte Profesional: stargazer
```{r results='asis', eval=FALSE}
stargazer(m1, m2, m3, 
          type = "text",
          title = "Determinantes del Salario",
          dep.var.labels = "Log(Salario)",
          covariate.labels = c("Educación", "Experiencia", "Antigüedad"),
          se = list(
            sqrt(diag(vcovHC(m1, type="HC1"))),
            sqrt(diag(vcovHC(m2, type="HC1"))),
            sqrt(diag(vcovHC(m3, type="HC1")))
          ),
          notes = "Errores estándar robustos entre paréntesis",
          notes.append = FALSE)
```

**Ventajas:**
- Formato profesional
- Múltiples modelos lado a lado
- Fácil de exportar a LaTeX, HTML, texto

---

# Tabla de Resultados
```{r echo=TRUE,eval = F, results='asis'}
library(modelsummary)
modelos <- list(
  "Modelo 1" = m1,
  "Modelo 2" = m2,
  "Modelo 3" = m3
)

modelsummary(modelos,output = '',
             vcov = "HC1",
             stars = TRUE,
             gof_map = c("nobs", "r.squared", "adj.r.squared"),
             coef_rename = c("educ" = "Educación",
                            "exper" = "Experiencia",
                            "tenure" = "Antigüedad"),
             title = "Determinantes del Log(Salario)")
```

---

# Checklist de Buenas Prácticas

✅ **SIEMPRE hacer:**

1. Explorar datos antes de modelar
2. Justificar elección de variables (teoría)
3. Reportar $R^2$ ajustado (no solo $R^2$)
4. Verificar multicolinealidad (VIF)
5. Hacer gráficos de residuos
6. Testear heteroscedasticidad
7. **Usar errores robustos en corte transversal**
8. Interpretar magnitud Y significancia
9. Ser honesto sobre limitaciones
10. Documentar todo tu código

---

# Checklist de Buenas Prácticas (cont.)

❌ **NUNCA hacer:**

1. Agregar variables solo para mejorar $R^2$
2. Eliminar observaciones sin justificación
3. Omitir variables relevantes conocidas
4. Ignorar diagnósticos
5. Interpretar correlación como causalidad sin justificación
6. Reportar solo resultados significativos (sesgo de publicación)
7. Olvidar la interpretación económica


---

# Ejemplo Integrador Completo
```{r}
# 1. DATOS
data(wage1)

# 2. EXPLORACIÓN
summary(wage1[, c("wage", "educ", "exper", "female")])
```

---

# Ejemplo Integrador (cont.)
```{r}
# 3. ESPECIFICACIÓN Y ESTIMACIÓN
modelo_final <- lm(log(wage) ~ educ + exper + I(exper^2) + female, 
                   data = wage1)

# 4. DIAGNÓSTICO
vif(modelo_final)
```

---

# Ejemplo Integrador (cont.)
```{r}
# Test de heteroscedasticidad
bptest(modelo_final)
```
```{r}
# 5. RESULTADOS CON ERRORES ROBUSTOS
coeftest(modelo_final, vcov = vcovHC(modelo_final, type = "HC1"))
```

---

# Ejemplo Integrador: Gráficos de Diagnóstico
```{r fig.height=6}
par(mfrow = c(2, 2))
plot(modelo_final)
```

---

# Interpretación Final del Modelo
```{r echo=FALSE}
summary(modelo_final)$coefficients
```

**Interpretaciones (en términos porcentuales):**

- **Educación:** +1 año → salario aumenta ~9.2%
- **Experiencia:** Efecto no lineal (cuadrático)
  - Inicialmente positivo, luego se aplana
  - Máximo en: $-\beta_{exper}/(2\times\beta_{exper^2}) = -0.041/(2\times-0.0007) \approx 29$ años
- **Género:** Mujeres ganan ~30% menos (manteniendo educación y experiencia constantes)

---

# Visualización del Efecto de Experiencia
```{r echo=FALSE, fig.height=5}
# Crear secuencia de experiencia
exper_seq <- seq(0, 50, by = 1)
# Predecir (manteniendo otras variables en su media)
pred_data <- data.frame(
  exper = exper_seq,
  educ = mean(wage1$educ),
  female = 0
)
pred_data$pred_wage <- predict(modelo_final, newdata = pred_data, type = "response")

ggplot(pred_data, aes(x = exper, y = exp(pred_wage))) +
  geom_line(color = "steelblue", size = 1.5) +
  labs(title = "Efecto de la Experiencia sobre el Salario (Hombres, educación promedio)",
       x = "Años de Experiencia",
       y = "Salario Predicho ($)") +
  theme_minimal(base_size = 14)
```

---

# Recursos Adicionales

**Paquetes útiles:**
- `car`: Diagnósticos (VIF, etc.)
- `lmtest`: Tests de especificación
- `sandwich`: Errores robustos
- `stargazer` / `modelsummary`: Tablas bonitas
- `ggplot2`: Visualizaciones
- `wooldridge`: Datasets de práctica

**Libros recomendados:**
- Wooldridge: "Introducción a la Econometría"
- Gujarati: "Econometría"
- Stock & Watson: "Introduction to Econometrics"

**Online:**
- R for Data Science: https://r4ds.had.co.nz/
- Cross Validated (Stack Exchange para estadística)

---

class: inverse, center, middle

# Resumen Final

---

# Puntos Clave para Recordar

**1. Interpretación:**
- Los coeficientes describen **asociaciones**, no necesariamente causalidad
- Diferentes formas funcionales tienen diferentes interpretaciones
- Mantener todo lo demás constante (ceteris paribus)

**2. Supuestos:**
- MCO requiere varios supuestos para ser óptimo
- Verificar siempre con diagnósticos
- Corregir cuando sea necesario (especialmente heteroscedasticidad)

**3. Práctica:**
- Explorar → Especificar → Estimar → Diagnosticar → Corregir → Interpretar
- Usar errores robustos por defecto en corte transversal
- Reportar de forma honesta y completa

---

class: inverse, center, middle

# ¿Preguntas?

---

class: center, middle

# ¡Gracias!

### Contacto: nsidicaro.fce@gmail.com